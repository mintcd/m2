{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7sePAkWpLJV"
      },
      "source": [
        "## Lab 4 - Part 3: Saving and loading models\n",
        "\n",
        "In this notebook, we'll see how to save and load models with TensorFlow. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data.\n",
        "\n",
        "# This part just show you how to save your best model for production environement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD856SqhH4JK"
      },
      "source": [
        "## Import resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e40KnjsVgUdy"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsu5egUUqPg9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm6g62fRgUdy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqsrWYDKp4Fd"
      },
      "outputs": [],
      "source": [
        "print('Using:')\n",
        "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
        "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAe81nXoICzC"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxcg_ZbuLnM3"
      },
      "outputs": [],
      "source": [
        "train_split = 60\n",
        "test_val_split = 20\n",
        "\n",
        "splits = ['train[:20%]+test[:20%]', 'train[20%:40%]+test[20%:40%]', 'train[40%:]+test[40%:]']\n",
        "\n",
        "dataset, dataset_info = tfds.load('fashion_mnist', split=splits, as_supervised=True, with_info=True)\n",
        "\n",
        "test_set, validation_set, training_set = dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set size: \", len(training_set))\n",
        "print(\"Valid set size: \", len(validation_set))\n",
        "print(\"Test set size:  \", len(test_set))"
      ],
      "metadata": {
        "id": "aAJnOfrSIrhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1WhOLC7Ii3D"
      },
      "source": [
        "## Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i2586KjI4QM"
      },
      "outputs": [],
      "source": [
        "total_examples = dataset_info.splits['train'].num_examples + dataset_info.splits['test'].num_examples\n",
        "\n",
        "num_training_examples = (total_examples * train_split) // 100\n",
        "num_validation_examples = (total_examples * test_val_split) // 100\n",
        "num_test_examples = num_validation_examples\n",
        "\n",
        "print('There are {:,} images in the training set'.format(num_training_examples))\n",
        "print('There are {:,} images in the validation set'.format(num_validation_examples))\n",
        "print('There are {:,} images in the test set'.format(num_test_examples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLMJCpppq43U"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeU9nb_xqW98"
      },
      "outputs": [],
      "source": [
        "for image, label in training_set.take(1):\n",
        "    image = image.numpy().squeeze()\n",
        "    label = label.numpy()\n",
        "\n",
        "plt.imshow(image, cmap=plt.cm.binary)\n",
        "plt.title(class_names[label])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5rUDqxBIt5N"
      },
      "source": [
        "## Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec3uphcyci3c"
      },
      "outputs": [],
      "source": [
        "def normalize(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255\n",
        "    return image, label\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "training_batches = training_set.cache().shuffle(num_training_examples//4).batch(batch_size).map(normalize).prefetch(1)\n",
        "validation_batches = validation_set.cache().batch(batch_size).map(normalize).prefetch(1)\n",
        "testing_batches = test_set.cache().batch(batch_size).map(normalize).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySQuJ-iPqNoR"
      },
      "source": [
        "## Build and train the model\n",
        "\n",
        "Here we'll build and compile our model as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Vnu0KJMqwc"
      },
      "outputs": [],
      "source": [
        "layer_neurons = [512, 256, 128]\n",
        "\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))\n",
        "\n",
        "for neurons in layer_neurons:\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation = 'relu'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qLJ-cAwnmFD"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 4\n",
        "\n",
        "history = model.fit(training_batches,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=validation_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jseIvfe2xb56"
      },
      "source": [
        "## Saving and loading models\n",
        "\n",
        "In TensorFlow we can save our trained models in different formats. Here we will see how to save our models in TensorFlow's SavedModel format and as HDF5 files, which is the format used by Keras models.\n",
        "\n",
        "### Saving and loading models in HDF5 format\n",
        "\n",
        "To save our models in the format used by Keras models we use the `.save(filepath)` method. For example, to save a model called `my_model` in the current working directory with the name `test_model` we use:\n",
        "\n",
        "```python\n",
        "my_model.save('./test_model.h5')\n",
        "```\n",
        "\n",
        "It's important to note that we have to provide the `.h5` extension to the `filepath` in order the tell `tf.keras` to save our model as an HDF5 file.\n",
        "\n",
        "The above command saves our model into a single HDF5 file that will contain:\n",
        "\n",
        "* The model's architecture.\n",
        "* The model's weight values which were learned during training.\n",
        "* The model's training configuration, which corresponds to the parameters you passed to the `compile` method.\n",
        "* The optimizer and its state. This allows you to resume training exactly where you left off.\n",
        "\n",
        "\n",
        "In the cell below we save our trained `model` as an HDF5 file. The name of our HDF5 will correspond to the current time stamp. This is useful if you are saving many models and want each of them to have a unique name. By default the `.save()` method will **silently** overwrite any existing file at the target location with the same name. If we want `tf.keras` to provide us with a manual prompt to whether overwrite files with the same name, you can set the argument `overwrite=False` in the `.save()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1dOvNRvrhNa"
      },
      "outputs": [],
      "source": [
        "t = time.time()\n",
        "\n",
        "saved_keras_model_filepath = './{}.h5'.format(int(t))\n",
        "\n",
        "model.save(saved_keras_model_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGNRBb1puSRg"
      },
      "source": [
        "Once a model has been saved, we can use `tf.keras.models.load_model(filepath)` to re-load our model. This command will also compile our model automatically using the saved training configuration, unless the model was never compiled in the first place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akaAVE2js5d0"
      },
      "outputs": [],
      "source": [
        "reloaded_keras_model = tf.keras.models.load_model(saved_keras_model_filepath)\n",
        "\n",
        "reloaded_keras_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWihP1oMjNeF"
      },
      "source": [
        "As we can see the re-loaded model has the same architecture as our original model, as it should be. At this point, since we haven't done anything new to the re-loaded model, then both the `reloaded_keras_model` our original `model` should be identical copies. Therefore, they should make the same predictions on the same images. Let's check that this is true:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLQsw7QVkElc"
      },
      "outputs": [],
      "source": [
        "for image_batch, label_batch in testing_batches.take(1):\n",
        "    prediction_1 = model.predict(image_batch)\n",
        "    prediction_2 = reloaded_keras_model.predict(image_batch)\n",
        "    difference = np.abs(prediction_1 - prediction_2)\n",
        "    print(difference.max())\n",
        "    print(image_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-dDOY0BmYhs"
      },
      "source": [
        "As we can see, the result is 0.0, which indicates that both models made the same predictions on the same images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FomAlrxnQnm8"
      },
      "source": [
        "## Saving models during training\n",
        "\n",
        "We have seen that when we train a model with a validation set, the value of the validation loss changes through the training process. Since the value of the validation loss is an indicator of how well our model will generalize to new data, it will be great if could save our model at each step of the training process and then only keep the version with the lowest validation loss.\n",
        "\n",
        "We can do this in `tf.keras` by using the following callback:\n",
        "\n",
        "```python\n",
        "tf.keras.callbacks.ModelCheckpoint('./best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "```\n",
        "This callback will save the model as a Keras file after every epoch. With the `save_best_only=True` argument, this callback will first check the validation loss of the latest model against the one previously saved. The callback will only save the latest model and overwrite the old one, if the latest model has a lower validation loss than the one previously saved. This will guarantee that will end up with the version of the model that achieved the lowest validation loss during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvsuAeUQ1WKR"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Stop training when there is no improvement in the validation loss for 10 consecutive epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Save the Model with the lowest validation loss\n",
        "save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.keras',\n",
        "                                               monitor='val_loss',\n",
        "                                               save_best_only=True)\n",
        "\n",
        "history = model.fit(training_batches,\n",
        "                    epochs = 100,\n",
        "                    validation_data=validation_batches,\n",
        "                    callbacks=[early_stopping, save_best])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGwKm4GVgUd1"
      },
      "source": [
        "## Try all steps above using your `CNN Network`\n",
        "Compare your results in DNN vs CNN network : you need to build and train your CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGG_Sv5sgUd1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awEAMAH7gUd1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdVrM7QegUd1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG2DHLRmgUd1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoEnuMOTgUd1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}