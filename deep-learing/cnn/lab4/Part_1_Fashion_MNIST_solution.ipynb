{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg2hLK7hlWdb"
      },
      "source": [
        "# Lab 4 - Part 1: Classifying Fashion-MNIST\n",
        "\n",
        "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1ZmqLpNTQSW2fhWaxtZRxlyd8G6BbN8Pa' width=400px> </center>\n",
        "\n",
        "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebooks though as you work through this.\n",
        "\n",
        "First of all, let's import our resources and download the Fashion-MNIST dataset from `tensorflow_datasets`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMflYTIOtOPf"
      },
      "source": [
        "## Import resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEVFxa8WYM_j"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0n2QWj1p2fG"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQyLYltaYM_k"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwP1_Qw-cCsY"
      },
      "outputs": [],
      "source": [
        "print('Using:')\n",
        "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
        "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr2SOjl8txrZ"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "We are now going to load the Fashion-MNIST dataset using `tensorflow_datasets` as we've done before. In this case, however, we are going to omit the `split` argument.  This means that `tensorflow_datasets` will use the default value for `split` which is `split=None`. When `split=None`, `tensorflow_datasets` returns a **dictionary** with all the splits available for the dataset you are loading. However, if the split is given explicitly, such as `split='train'`, then `tensorflow_datasets` returns a `tf.data.Dataset` object.\n",
        "\n",
        "In our case, we are going to load the `fashion_mnist` dataset. If we look at the [documentation](https://www.tensorflow.org/datasets/catalog/fashion_mnist#statistics) we will see that this particular dataset has 2 splits, namely a `train` and a `test` slipt. We also see that the `train` split has 60,000 examples, and that the `test` split has 10,000 examples.\n",
        "\n",
        "Now, let's load the `fashion_mnist` dataset and inspect the returned values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kn4Op7dXCnk"
      },
      "outputs": [],
      "source": [
        "dataset, dataset_info = tfds.load('fashion_mnist', as_supervised = True, with_info = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_vT6HUUXg05"
      },
      "outputs": [],
      "source": [
        "# Check that dataset is a dictionary\n",
        "print('dataset has type:', type(dataset))\n",
        "\n",
        "# Print the keys of the dataset dictionary\n",
        "print('\\nThe keys of dataset are:', list(dataset.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S4f2J9jbpak"
      },
      "source": [
        "In the cell below, we are going to save the training data and the test data into different variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxo7PHJys18t"
      },
      "outputs": [],
      "source": [
        "training_set, test_set = dataset['train'], dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzZciG_KcHbI"
      },
      "source": [
        "Now, let's take a look at the `dataset_info`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jFE3vbebU-A"
      },
      "outputs": [],
      "source": [
        "# Display the dataset_info\n",
        "dataset_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_If36cti685"
      },
      "source": [
        "We can access the information in `dataset_info` very easily. As we can see, the `features` and `splits` info are contained in dictionaries. We can access the information we want by accessing the particular key and value in these dictionaries. We start by looking at the values of particular keys in these dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiOf0aqlYM_l"
      },
      "outputs": [],
      "source": [
        "dataset_info.features['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n1acmAbYM_l"
      },
      "outputs": [],
      "source": [
        "dataset_info.features['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA3Q-1ktYM_l"
      },
      "outputs": [],
      "source": [
        "dataset_info.splits['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsDR7iOVYM_m"
      },
      "outputs": [],
      "source": [
        "dataset_info.splits['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFwhpPOijumG"
      },
      "source": [
        "We can now use `dot` notation to access the information we want. Below are some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9_OYPHsbbcl"
      },
      "outputs": [],
      "source": [
        "shape_images = dataset_info.features['image'].shape\n",
        "num_classes = dataset_info.features['label'].num_classes\n",
        "\n",
        "num_training_examples  = dataset_info.splits['train'].num_examples\n",
        "num_test_examples = dataset_info.splits['test'].num_examples\n",
        "\n",
        "print('There are {:,} classes in our dataset.'.format(num_classes))\n",
        "print('The images in our dataset have shape:', shape_images)\n",
        "\n",
        "print('\\nThere are {:,} images in the test set.'.format(num_test_examples))\n",
        "print('There are {:,} images in the training set.'.format(num_training_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfMgIb3PvWXo"
      },
      "source": [
        "## Explore the dataset\n",
        "\n",
        "The images in this dataset are 28 $\\times$ 28 arrays, with pixel values in the range `[0, 255]`. The *labels* are an array of integers, in the range `[0, 9]`. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Each image is mapped to a single label. Since the *class names* are not included with the dataset, we create them here to use later when plotting the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odzN3aJjusED"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoY1HeJJyces"
      },
      "outputs": [],
      "source": [
        "for image, label in training_set.take(1):\n",
        "    print('The images in the training set have:\\n\\u2022 dtype:', image.dtype, '\\n\\u2022 shape:', image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CInprnnJ1_gk"
      },
      "outputs": [],
      "source": [
        "for image, label in training_set.take(1):\n",
        "    image = image.numpy().squeeze()\n",
        "    label = label.numpy()\n",
        "\n",
        "plt.imshow(image, cmap= plt.cm.binary)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print('The label of this image is:', label)\n",
        "print('The class name of this image is:', class_names[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb-lmuTM35C9"
      },
      "source": [
        "## Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gq-_mXl3ZFG"
      },
      "outputs": [],
      "source": [
        "def normalize(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255\n",
        "    return image, label\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "training_batches = training_set.cache().shuffle(num_training_examples//4).batch(batch_size).map(normalize).prefetch(1)\n",
        "testing_batches = test_set.cache().batch(batch_size).map(normalize).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LviX4-ii8js7"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "> **Exercise:** Here you should define your own neural network. Feel free to create a model with as many layers and neurons as you like. You should keep in mind that as with MNIST, each image is 28 $\\times$ 28 which is a total of 784 pixels, and there are 10 classes. Your model should include at least one hidden layer. We suggest you use ReLU activation functions for the hidden layers and a softmax activation function for the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYzFZ3jQ8azd"
      },
      "outputs": [],
      "source": [
        "## Solution\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
        "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYhwsFzA-Aah"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "> **Exercise:** Compile the model you created above using an `adam` optimizer, a `sparse_categorical_crossentropy` loss function, and the `accuracy` metric. Then train the model for 5 epochs. You should be able to get the training loss below 0.4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyy9SqTU91IS"
      },
      "outputs": [],
      "source": [
        "## Solution\n",
        "model.compile(optimizer= 'adam',\n",
        "              loss= 'sparse_categorical_crossentropy',\n",
        "              metrics= ['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(training_batches,epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REJbwplUBoRT"
      },
      "source": [
        "## Evaluate loss and accuracy on the test set\n",
        "\n",
        "Now let's see how the model performs on the test set. This time, we will use all the examples in our test set to assess the loss and accuracy of our model. Remember, the images in the test are images the model has never seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q76aDGGl_xp4"
      },
      "outputs": [],
      "source": [
        "# your solution here\n",
        "loss, accuracy = model.evaluate(testing_batches)\n",
        "\n",
        "print('\\nLoss on the TEST Set: {:,.3f}'.format(loss))\n",
        "print('Accuracy on the TEST Set: {:.3%}'.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnpZWDQp2Zaq"
      },
      "source": [
        "## Check predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqUzc4pYAe7Z"
      },
      "outputs": [],
      "source": [
        "for image_batch, label_batch in testing_batches.take(1):\n",
        "    ps = model.predict(image_batch)\n",
        "    first_image = image_batch.numpy().squeeze()[0]\n",
        "    first_label = label_batch.numpy()[0]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
        "ax1.axis('off')\n",
        "ax1.set_title(class_names[first_label])\n",
        "ax2.barh(np.arange(10), ps[0])\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(class_names, size='small');\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try all steps above using your `CNN Network`\n",
        "Compare your results in DNN vs CNN network : you need to build and train your CNN"
      ],
      "metadata": {
        "id": "OA34NavohwTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbXRz5MTYM_q"
      },
      "outputs": [],
      "source": [
        "#Build CNN Network\n",
        "cnn_model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape = (28, 28, 1),padding='same'),\n",
        "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
        "                tf.keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same'),\n",
        "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
        "                tf.keras.layers.Conv2D(128, (3, 3), activation='relu',padding='same'),\n",
        "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(128, activation='relu'),\n",
        "                tf.keras.layers.Dense(32, activation='relu'),\n",
        "                tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "cnn_model.compile(optimizer= 'adam',\n",
        "              loss= 'sparse_categorical_crossentropy',\n",
        "              metrics= ['accuracy']\n",
        ")\n",
        "\n",
        "cnn_model.fit(training_batches,epochs=5)"
      ],
      "metadata": {
        "id": "fnNeiNL_uq5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalute loss and accuracy on test set\n",
        "cnn_loss, cnn_accuracy = cnn_model.evaluate(testing_batches)\n",
        "\n",
        "print('\\nLoss on the TEST Set: {:,.3f}'.format(cnn_loss))\n",
        "print('Accuracy on the TEST Set: {:.3%}'.format(cnn_accuracy))"
      ],
      "metadata": {
        "id": "0I6zVp-PusXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check predictions\n",
        "for image_batch, label_batch in testing_batches.take(1):\n",
        "    cnn_ps = cnn_model.predict(image_batch)\n",
        "    first_image = image_batch.numpy().squeeze()[0]\n",
        "    first_label = label_batch.numpy()[0]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
        "ax1.axis('off')\n",
        "ax1.set_title(class_names[first_label])\n",
        "ax2.barh(np.arange(10), cnn_ps[0])\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(class_names, size='small');\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "DToqKIIIu2gX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}