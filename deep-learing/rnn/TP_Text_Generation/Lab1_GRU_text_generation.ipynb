{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mZ6hb_uY34Z"
   },
   "source": [
    "<img src=\"./image/labai.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otztqvtkY34b"
   },
   "source": [
    "# Text Generation with GRU\n",
    "\n",
    "In this exercise your goal is to build text generation model with GRU model by complete all piece of code below, you can add or change code as we can\n",
    "\n",
    "\n",
    "**Objective**:  \n",
    "In this exercise, your goal is to build a text generation model using a Gated Recurrent Unit (GRU). You will complete all the provided code segments and are encouraged to add or modify code to improve the model. The key steps involve:\n",
    "\n",
    "1. Preprocessing the text data.\n",
    "2. Implementing the GRU-based neural network.\n",
    "3. Training the model on the provided dataset.\n",
    "4. Generating new text based on a seed sequence.\n",
    "\n",
    "**Instructions**:\n",
    "- Follow the code structure provided and complete the missing sections.\n",
    "- Experiment with different hyperparameters to improve performance.\n",
    "- You are free to adjust the code as needed to enhance results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdNrLT0GY34c",
    "outputId": "b29c40b6-3d1b-482d-cdc6-83a0c6224df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from typing import List,Dict\n",
    "\n",
    "# import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import unicodedata\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab  import build_vocab_from_iterator\n",
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rqAjwZ1TY34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "text = Path('./data/tiny-shakespeare.txt').read_text()\n",
    "print(f'Number of characters in text file: {len(text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwwOe-tJ-xcE",
    "outputId": "15c97619-b3b8-486e-c482-1917d91b70c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeHkhOxoY34f"
   },
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4uwdls7xY34f"
   },
   "outputs": [],
   "source": [
    "class  WordTokenizer(nn.Module):\n",
    "    def __init__(self, vocab: torchtext.vocab.Vocab|Dict[str,int])-> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(vocab, torchtext.vocab.Vocab):\n",
    "            self.token2id=vocab.get_stoi()\n",
    "            self.id2token={id:ch for ch,id in vocab.get_stoi().items()}\n",
    "            self.vocab_size=len(self.token2id)\n",
    "\n",
    "        elif isinstance(vocab, dict):\n",
    "            self.token2id=vocab\n",
    "            self.id2token={id:ch for ch,id in vocab.items()}\n",
    "            self.vocab_size=len(self.token2id)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Please loads a vocabulary file into a dictionary \\\n",
    "                            Dict[str,int] or torchtext.vocab.Vocab\")\n",
    "\n",
    "    def encode(self, text:List[str]|str):\n",
    "        if isinstance(text, str):\n",
    "            text_list=self.tokenize(text)\n",
    "\n",
    "        token_id=[]\n",
    "        for token in text_list:\n",
    "            token_id.append(self.token2id[token])\n",
    "        return  torch.tensor(token_id,  dtype=torch.long)\n",
    "\n",
    "\n",
    "    def decode(self, idx:torch.tensor):\n",
    "        #idx: torch.Tensor containing integers\n",
    "        token=[]\n",
    "        for id in idx.tolist():\n",
    "            token.append(self.id2token[id])\n",
    "        return ' '.join(token)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "\n",
    "        # Normalize incoming text; can be multiple actions\n",
    "        text= text.lower() ## Your code Here ##\n",
    "\n",
    "        # split text into tokens\n",
    "        tokens= re.split(r'\\W+', text) ## Your code Here ##\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def _tokenizer_corpus(corpus:List[str]):\n",
    "        for text in corpus:\n",
    "            yield WordTokenizer.tokenize(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def train_from_text(text: str) -> List[str]:\n",
    "        \"\"\"build vocab from one text corpus\"\"\"\n",
    "        vocab=build_vocab_from_iterator(WordTokenizer._tokenizer_corpus(WordTokenizer.tokenize(text)),\n",
    "                                        specials=[\"<unk>\"]\n",
    "                                       )\n",
    "        vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "        return WordTokenizer(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YGIb4sShY34f"
   },
   "outputs": [],
   "source": [
    "# create tokenizer from text\n",
    "tokenizer = WordTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTjLFATYY34f",
    "outputId": "1a1d6613-10d8-4654-88a2-f736e6099607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish', 'all', 'resolved', 'resolved', 'first', 'citizen', 'first', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people', 'all', 'we', 'know', 't', 'we', 'know', 't', 'first', 'citizen', 'let', 'us']\n"
     ]
    }
   ],
   "source": [
    "# show example of word-based tokens\n",
    "print(tokenizer.tokenize(text[0:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qwt6OCMsY34f",
    "outputId": "f5a75310-d3b4-4cb7-ec69-af1715e4bd74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 304,    4,    1,  553, 3000,  659, 6539])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "encode_text=tokenizer.encode(\"Welcome to the deep learning course.\")\n",
    "encode_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Min8ga3iY34g",
    "outputId": "e908e79d-55a3-425b-99dd-8c9709f280ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to the deep learning course '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text=tokenizer.decode(encode_text)\n",
    "decode_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tklWrh1JY34g"
   },
   "source": [
    "#### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6c0xseC9Y34g"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, encode_text, max_seq_length: int):\n",
    "        self.encode_text     = encode_text\n",
    "        self.max_seq_length  = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encode_text)-self.max_seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self.encode_text)-self.max_seq_length\n",
    "\n",
    "        x_train= self.encode_text[idx:idx+self.max_seq_length]\n",
    "\n",
    "        # Target is shifted by one character/token\n",
    "        y_target= self.encode_text[idx+1:idx+1+self.max_seq_length]\n",
    "\n",
    "        return x_train, y_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTOkrUplY34g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ShakespeareDataset(encode_text=tokenizer.encode(text),max_seq_length=100)\n",
    "len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "u1laQ2pnY34g",
    "outputId": "fee87a1d-f424-4185-fb31-d7597b2e293f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we know t we know t first citizen let us kill him and we ll have corn at our own price is t a verdict all no more talking on t let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "mXi5HOKoY34g",
    "outputId": "7d939f13-eb03-44d9-90e7-459db296ce14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we know t we know t first citizen let us kill him and we ll have corn at our own price is t a verdict all no more talking on t let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mI9yTtUlY34g"
   },
   "outputs": [],
   "source": [
    "# batch dataset\n",
    "train_dataloader = DataLoader(dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in train_dataloader:\n",
    "  print(X_batch, y_batch)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eld3OBKoY34g"
   },
   "source": [
    "## Build GRU model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "eC6I6cNLY34h"
   },
   "outputs": [],
   "source": [
    "class GRUTextGen(nn.Module):\n",
    "    def __init__(self,\n",
    "                 tokenizer: WordTokenizer,\n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 num_layers, \n",
    "                 dropout):\n",
    "        super(GRUTextGen, self).__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self. num_layers= num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n",
    "\n",
    "        self.gru = nn.GRU(input_size=self.embedding_dim,\n",
    "                          hidden_size=self.hidden_dim,\n",
    "                          num_layers=self. num_layers,\n",
    "                          dropout=self.dropout,\n",
    "                          batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=self.vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        assert x.ndim==2  # x tensor must be 2D dimensions with shape (B,S), B=batch, S=sequence length\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.gru(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "    \n",
    "    def fit(self, epochs = 5, \n",
    "                  from_epoch = 0,\n",
    "                  optimizer=optim.Adam,\n",
    "                  lr=0.001,\n",
    "                  loss_function = nn.CrossEntropyLoss()):\n",
    "        \n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(from_epoch, epochs+from_epoch):\n",
    "          # Set model into \"training mode\"\n",
    "          self.train()\n",
    "          total_loss = 0\n",
    "\n",
    "          count = 0\n",
    "          for X_batch, y_batch in train_dataloader:\n",
    "              X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "              optimizer.zero_grad()\n",
    "              output = self(X_batch)\n",
    "              loss = loss_function(output.view(-1, output.size(-1)), y_batch.view(-1))\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "              total_loss += loss.item()\n",
    "\n",
    "              count += 1\n",
    "              if count % 100 == 0:\n",
    "                print(f'Batch {count}/{len(train_dataloader)}, Loss: {total_loss / len(train_dataloader)}')\n",
    "                model_name = f'rnn_epoch_{epoch+1}_batch_{count}.net'\n",
    "\n",
    "                with open(model_name, 'wb') as f:\n",
    "                    torch.save(self.state_dict(), f)\n",
    "\n",
    "          print(f'Epoch {epoch + 1}/{epochs+from_epoch}, Loss: {total_loss / len(train_dataloader)}')\n",
    "\n",
    "          path = f'rnn_epoch_{epoch+1}_completed.net'\n",
    "          with open(path, 'wb') as f:\n",
    "            torch.save(self.state_dict(), f)\n",
    "\n",
    "          print(\"Sample output:\")\n",
    "\n",
    "          gen_output = self.predict(\n",
    "              temperature=0.8,\n",
    "              max_tokens=30,\n",
    "              top_k=None,\n",
    "              do_sample=False\n",
    "          )\n",
    "          print(gen_output)\n",
    "\n",
    "          print('-'*72)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, \n",
    "                input_text: str = \"To be or not to be\", \n",
    "                max_tokens: int = 15, \n",
    "                temperature: float = 1, \n",
    "                top_k: int = None, \n",
    "                do_sample: bool = False):\n",
    "        \"\"\"Inference: Define Text Generation\"\"\"\n",
    "        \n",
    "        idx = self.tokenizer.encode(input_text).unsqueeze(dim=0).to(DEVICE)\n",
    "        max_sequence_length = 31\n",
    "\n",
    "        assert idx.ndim == 2, \"input token must be 2D with shape (B, S)\"\n",
    "\n",
    "        for _ in range(max_tokens):  # Maximum number of tokens to generate\n",
    "            # Limit the context length to max_sequence_length\n",
    "            idx_cond = idx if idx.size(1) <= max_sequence_length else idx[:, -max_sequence_length:]\n",
    "\n",
    "            # Forward pass through the model\n",
    "            logits = self(idx_cond)\n",
    "\n",
    "            # Get logits of the last token and scale by temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k is not None:\n",
    "                values = torch.topk(logits, top_k).values\n",
    "                logits[logits < values[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample from probabilities or pick the most likely word\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.topk(probs, k=1, dim=-1).indices  # Greedy decoding\n",
    "\n",
    "            # Append the new token index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return self.tokenizer.decode(idx.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUTextGen(\n",
       "  (tokenizer): WordTokenizer()\n",
       "  (embedding): Embedding(11458, 128)\n",
       "  (gru): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=256, out_features=11458, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "gru = GRUTextGen(tokenizer=tokenizer,\n",
    "                 embedding_dim=embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 num_layers=num_layers, \n",
    "                 dropout=dropout)\n",
    "gru.to(DEVICE)\n",
    "gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to be or not to be circumstances sail brotherly industriously borough borough shed shed bruising nothings nothings conjures saves prime proverb supporters cacodemon flattered statue gall statue gall statue sinks gall behoveful mildly avoiding absolver happened'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a prediction before training\n",
    "gru.predict(temperature=0.8,\n",
    "            max_tokens=30,\n",
    "            top_k=None,\n",
    "            do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/815, Loss: 0.8768252279129496\n",
      "Batch 200/815, Loss: 1.7321505084359572\n",
      "Batch 300/815, Loss: 2.582938046835683\n",
      "Batch 400/815, Loss: 3.4312840175043586\n",
      "Batch 500/815, Loss: 4.27374253945848\n",
      "Batch 600/815, Loss: 5.127304993962949\n",
      "Batch 700/815, Loss: 5.959392193051204\n",
      "Batch 800/815, Loss: 6.810870385901328\n",
      "Epoch 1/5, Loss: 6.942598291993873\n",
      "Sample output:\n",
      "to be or not to be the the the the the antonio the the the the the the the the the the the the the the the the the the the the antonio antonio antonio the\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.8244261805996573\n",
      "Batch 200/815, Loss: 1.6529137313000264\n",
      "Batch 300/815, Loss: 2.486510784055558\n",
      "Batch 400/815, Loss: 3.3187229174046426\n",
      "Batch 500/815, Loss: 4.1496200590777255\n",
      "Batch 600/815, Loss: 4.984988954462157\n",
      "Batch 700/815, Loss: 5.800949736753124\n",
      "Batch 800/815, Loss: 6.632969411019167\n",
      "Epoch 2/5, Loss: 6.762181098914585\n",
      "Sample output:\n",
      "to be or not to be the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.8291349247189387\n",
      "Batch 200/815, Loss: 1.6591416113215722\n",
      "Batch 300/815, Loss: 2.493678745901658\n",
      "Batch 400/815, Loss: 3.325144581121901\n",
      "Batch 500/815, Loss: 4.154360307213719\n",
      "Batch 600/815, Loss: 4.981081094332268\n",
      "Batch 700/815, Loss: 5.77566583258974\n",
      "Batch 800/815, Loss: 6.569753193416479\n",
      "Epoch 3/5, Loss: 6.69169123011864\n",
      "Sample output:\n",
      "to be or not to be a island of the island of the island of the island of the island of the island of the island of the island of the island of the island of\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.774233421957566\n",
      "Batch 200/815, Loss: 1.538160519512153\n",
      "Batch 300/815, Loss: 2.301908710690364\n",
      "Batch 400/815, Loss: 3.063812404468747\n",
      "Batch 500/815, Loss: 3.8171369505806205\n",
      "Batch 600/815, Loss: 4.5704023250041566\n",
      "Batch 700/815, Loss: 5.290555145989167\n",
      "Batch 800/815, Loss: 6.010688671158866\n",
      "Epoch 4/5, Loss: 6.121954116353228\n",
      "Sample output:\n",
      "to be or not to be a man of the island of the sea of the sea of the island of the sea of the sea of the sea of the sea of naples gonzalo and\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.7088666266458897\n",
      "Batch 200/815, Loss: 1.4039179380686004\n",
      "Batch 300/815, Loss: 2.1010589886296747\n",
      "Batch 400/815, Loss: 2.80125749038041\n",
      "Batch 500/815, Loss: 3.4899393005605126\n",
      "Batch 600/815, Loss: 4.188756395702713\n",
      "Batch 700/815, Loss: 4.861687086257467\n",
      "Batch 800/815, Loss: 5.536351189876626\n",
      "Epoch 5/5, Loss: 5.639990893755954\n",
      "Sample output:\n",
      "to be or not to be a man of naples and all the sea of the sea of naples and the king s ship the wreck and the earth of naples sebastian i have heard d\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gru.fit(epochs=5, \n",
    "        optimizer=optim.Adam, \n",
    "        lr=0.001, \n",
    "        loss_function=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is still to high and the prediction does not make much sense. Let's reload the model and train for 5 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru = GRUTextGen(tokenizer=tokenizer,\n",
    "                 embedding_dim=embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 num_layers=num_layers, \n",
    "                 dropout=dropout)\n",
    "gru.to(DEVICE)\n",
    "with open('rnn_epoch_5_completed.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "gru.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.fit(epochs=5, from_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/815, Loss: 0.542288696546496\n",
      "Batch 200/815, Loss: 1.097818767805041\n",
      "Batch 300/815, Loss: 1.6562972601206025\n",
      "Batch 400/815, Loss: 2.234626911607988\n",
      "Batch 500/815, Loss: 2.7985698185084056\n",
      "Batch 600/815, Loss: 3.3807567415061905\n",
      "Batch 700/815, Loss: 3.9525335727293798\n",
      "Batch 800/815, Loss: 4.515815020485158\n",
      "Epoch 11/20, Loss: 4.601973138996429\n",
      "Sample output:\n",
      "to be or not to be obedient prospero thou hast miranda thee to thy island prospero thou shalt be done to t thou shalt be miranda to be t prospero thou shalt be miranda and thou\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.5199754252755568\n",
      "Batch 200/815, Loss: 1.0334029964142781\n",
      "Batch 300/815, Loss: 1.5471124453047302\n",
      "Batch 400/815, Loss: 2.0778643593466355\n",
      "Batch 500/815, Loss: 2.5972030481677844\n",
      "Batch 600/815, Loss: 3.1262523224017373\n",
      "Batch 700/815, Loss: 3.648750660609614\n",
      "Batch 800/815, Loss: 4.161514106411144\n",
      "Epoch 12/20, Loss: 4.239510480026526\n",
      "Sample output:\n",
      "to be or not to be obedient prospero thou hast a slave ariel away thou didst painfully the issue had been spoken to the end of thy life thou wast thy brother s son and brought\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.49404480120887057\n",
      "Batch 200/815, Loss: 0.9767655723665389\n",
      "Batch 300/815, Loss: 1.4568462131944901\n",
      "Batch 400/815, Loss: 1.953985966934017\n",
      "Batch 500/815, Loss: 2.441908593558095\n",
      "Batch 600/815, Loss: 2.9347898527157086\n",
      "Batch 700/815, Loss: 3.424129293301354\n",
      "Batch 800/815, Loss: 3.9028546005670277\n",
      "Epoch 13/20, Loss: 3.9750813416908124\n",
      "Sample output:\n",
      "to be or not to be obedient prospero thou hast a slave ariel prospero thou art too rich miranda thou hast a slave ariel to thee ariel prospero thy father s death and thy son thou\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.47051303372061326\n",
      "Batch 200/815, Loss: 0.9280919566476272\n",
      "Batch 300/815, Loss: 1.3806791539572498\n",
      "Batch 400/815, Loss: 1.851145526967897\n",
      "Batch 500/815, Loss: 2.313480729705717\n",
      "Batch 600/815, Loss: 2.778519593572324\n",
      "Batch 700/815, Loss: 3.241975472748645\n",
      "Batch 800/815, Loss: 3.6946819791033225\n",
      "Epoch 14/20, Loss: 3.7625380858321864\n",
      "Sample output:\n",
      "to be or not to be obedient prospero i prithee prospero prospero go to me prospero thou liest malignant slave prospero i have no more but prospero o sir what is the matter prospero prospero prospero\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.45052995155194053\n",
      "Batch 200/815, Loss: 0.8878061475929308\n",
      "Batch 300/815, Loss: 1.3182114721075888\n",
      "Batch 400/815, Loss: 1.7673726479700007\n",
      "Batch 500/815, Loss: 2.208894153314134\n",
      "Batch 600/815, Loss: 2.652231016919657\n",
      "Batch 700/815, Loss: 3.0954891418386823\n",
      "Batch 800/815, Loss: 3.528034874266642\n",
      "Epoch 15/20, Loss: 3.592530385116858\n",
      "Sample output:\n",
      "to be or not to be obedient prospero he shall be done prospero i ll not have thee but prospero my heart ariel prospero thou didst vent didst thou didst kill me prospero prospero thou hast\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.43290641644249667\n",
      "Batch 200/815, Loss: 0.8524456644350765\n",
      "Batch 300/815, Loss: 1.264422069736785\n",
      "Batch 400/815, Loss: 1.6965679560702271\n",
      "Batch 500/815, Loss: 2.1208540419128044\n",
      "Batch 600/815, Loss: 2.5460366576727185\n",
      "Batch 700/815, Loss: 2.9725710055579437\n",
      "Batch 800/815, Loss: 3.3882406884175866\n",
      "Epoch 16/20, Loss: 3.4500508873009244\n",
      "Sample output:\n",
      "to be or not to be obedient in the midst of the king s son is the event of the volsces ariel prospero the good hearts of the volsces but thou shalt be infused and i\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.41697115313056055\n",
      "Batch 200/815, Loss: 0.8213775541153422\n",
      "Batch 300/815, Loss: 1.216816798309607\n",
      "Batch 400/815, Loss: 1.6341184285520776\n",
      "Batch 500/815, Loss: 2.0430953379789014\n",
      "Batch 600/815, Loss: 2.452729300048454\n",
      "Batch 700/815, Loss: 2.865402038843354\n",
      "Batch 800/815, Loss: 3.266340492693193\n",
      "Epoch 17/20, Loss: 3.3256217447526617\n",
      "Sample output:\n",
      "to be or not to be obedient in the midst of the street but what is t what says your daughter is the fault and what i do begins to the people of the isle and\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.4027655282634899\n",
      "Batch 200/815, Loss: 0.7937573070175077\n",
      "Batch 300/815, Loss: 1.175164377469958\n",
      "Batch 400/815, Loss: 1.5793113018106097\n",
      "Batch 500/815, Loss: 1.974778894412737\n",
      "Batch 600/815, Loss: 2.370894158544716\n",
      "Batch 700/815, Loss: 2.7713888065946612\n",
      "Batch 800/815, Loss: 3.1592591815199587\n",
      "Epoch 18/20, Loss: 3.2163719857397255\n",
      "Sample output:\n",
      "to be or not to be obedient for he is the mad and he that loves him in a good marriage of naples sebastian he is a sailmaker of naples and he is to be constant\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.390038686588498\n",
      "Batch 200/815, Loss: 0.7688448309167031\n",
      "Batch 300/815, Loss: 1.1373707192075764\n",
      "Batch 400/815, Loss: 1.5303639815628893\n",
      "Batch 500/815, Loss: 1.9141983649482024\n",
      "Batch 600/815, Loss: 2.2983329547694855\n",
      "Batch 700/815, Loss: 2.6879312968692894\n",
      "Batch 800/815, Loss: 3.064168742244229\n",
      "Epoch 19/20, Loss: 3.119284920019606\n",
      "Sample output:\n",
      "to be or not to be so sebastian what s he that s not nor the right of the king nor he will make the king s ship he is to be obedient sebastian then is\n",
      "------------------------------------------------------------------------\n",
      "Batch 100/815, Loss: 0.37925525911015234\n",
      "Batch 200/815, Loss: 0.7475335820320925\n",
      "Batch 300/815, Loss: 1.104579957716304\n",
      "Batch 400/815, Loss: 1.486222719707372\n",
      "Batch 500/815, Loss: 1.859721803665161\n",
      "Batch 600/815, Loss: 2.2331452656377313\n",
      "Batch 700/815, Loss: 2.612140917339208\n",
      "Batch 800/815, Loss: 2.9776773835983743\n",
      "Epoch 20/20, Loss: 3.0309364096518676\n",
      "Sample output:\n",
      "to be or not to be obedient i am constant to have no more than shoes or rather thou canst know st t not thou didst thyself to chide the business in the time thou didst\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gru = GRUTextGen(tokenizer=tokenizer,\n",
    "                 embedding_dim=embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 num_layers=num_layers, \n",
    "                 dropout=dropout)\n",
    "gru.to(DEVICE)\n",
    "with open('rnn_epoch_10_completed.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "gru.load_state_dict(checkpoint)\n",
    "gru.fit(epochs=10, from_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reconstruct the losses of the model for every epoch to see training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 1: 6.950430837876958\n",
      "Loss of epoch 2: 6.947858932120669\n",
      "Loss of epoch 3: 6.690801437939603\n",
      "Loss of epoch 4: 6.100714593430969\n",
      "Loss of epoch 5: 5.840942907625912\n",
      "Loss of epoch 6: 5.750919371294829\n",
      "Loss of epoch 7: 5.512190454869183\n",
      "Loss of epoch 8: 5.277867445916486\n",
      "Loss of epoch 9: 5.044462486571329\n",
      "Loss of epoch 10: 4.858134903644491\n",
      "Loss of epoch 11: 4.950546730661685\n",
      "Loss of epoch 12: 4.739075697565371\n",
      "Loss of epoch 13: 4.55750772558107\n",
      "Loss of epoch 14: 4.455636697020267\n",
      "Loss of epoch 15: 4.3377204315794025\n",
      "Loss of epoch 16: 4.181216391025146\n",
      "Loss of epoch 17: 4.0463507109624475\n",
      "Loss of epoch 18: 3.9558619196429574\n",
      "Loss of epoch 19: 3.8999819382568077\n",
      "Loss of epoch 20: 3.872848076908135\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "gru = GRUTextGen(tokenizer=tokenizer,\n",
    "                   embedding_dim=embedding_dim, \n",
    "                   hidden_dim=hidden_dim, \n",
    "                   num_layers=num_layers, \n",
    "                   dropout=dropout)\n",
    "gru.to(DEVICE)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "  with open(f'rnn_epoch_{epoch}_completed.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "  gru.load_state_dict(checkpoint)\n",
    "  total_loss = 0\n",
    "\n",
    "  with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "      for x_train, y_target in train_dataloader:\n",
    "          # Move tensors to the correct device (GPU or CPU)\n",
    "          x_train, y_target = x_train.to(DEVICE), y_target.to(DEVICE)\n",
    "          output = gru(x_train)  # Shape: [batch_size, seq_length, vocab_size]\n",
    "          loss = criterion(output.view(-1, output.shape[-1]), y_target.view(-1))\n",
    "\n",
    "          # Accumulate the loss\n",
    "          total_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over all samples\n",
    "  average_loss = total_loss / len(train_dataloader)\n",
    "  print(f\"Loss of epoch {epoch}: {average_loss}\")\n",
    "  loss_history.append(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the final model to `rnn_final.net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru = GRUTextGen(tokenizer=tokenizer,\n",
    "                   embedding_dim=embedding_dim, \n",
    "                   hidden_dim=hidden_dim, \n",
    "                   num_layers=num_layers, \n",
    "                   dropout=dropout)\n",
    "gru.to(DEVICE)\n",
    "with open(f'rnn_final.net', 'rb') as f:\n",
    "  checkpoint = torch.load(f)\n",
    "\n",
    "gru.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to be or not to be obedient sebastian so please you sir i am no prospero but to be endured prospero i have done so much more long of his love to be ta en from me prospero that i have been to have her in which will not have it wiselier no nor no my good lord prospero i have prospero that i have done mine own is my consent and mine my slave my spirit is more than my fancy miranda my ass miranda yes lucentio thou canst not choose prospero miranda miranda sir prospero sir prospero sir what cerns you of my master'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.predict(input_text=\"To be or not to be\",\n",
    "            max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: despite limitations on dataset and training resource, the model generates nearly grammatical correct sample."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
