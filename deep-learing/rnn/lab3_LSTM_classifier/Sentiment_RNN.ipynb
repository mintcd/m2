{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN\n",
    "\n",
    "In this notebook, you'll implement a recurrent neural network that performs sentiment analysis. Using an RNN rather than a feedfoward network is more accurate since we can include information about the *sequence* of words. Here we'll use a dataset of movie reviews, accompanied by labels.\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "Here, we'll pass in words to an embedding layer. We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. You should have seen this before from the word2vec lesson. You can actually train up an embedding with word2vec and use it here. But it's good enough to just have an embedding layer and let the network learn the embedding table on it's own.\n",
    "\n",
    "From the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data. Finally, the LSTM cells will go to a sigmoid output layer here. We're using the sigmoid because we're trying to predict if this text has positive or negative sentiment. The output layer will just be a single unit then, with a sigmoid activation function.\n",
    "\n",
    "We don't care about the sigmoid outputs except for the very last one, we can ignore the rest. We'll calculate the cost from the output of the last step and the training label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./sentiment-network/reviews.txt', 'r') as f:\n",
    "    reviews = f.readlines()\n",
    "with open('./sentiment-network/labels.txt', 'r') as f:\n",
    "    labels = f.readlines()\n",
    "\n",
    "reviews = [review.strip() for review in reviews]\n",
    "labels = [label.strip() for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 2633\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(review.split()) for review in reviews])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training data\n",
    "1. Encode words into integers\n",
    "2. Reconcile each integer list to length 200 by slice longer list and padding shorter list\n",
    "\n",
    "For labels\n",
    "1. Encode positive to 1 and negative to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "  def __init__(self, vocab, max_length=200):\n",
    "    self.vocab = vocab\n",
    "    self.max_length = max_length\n",
    "    self.vocab_to_int = {word: index+1 for index, word in enumerate(vocab)}\n",
    "  \n",
    "  def encode(self, text):\n",
    "    text_int = [self.vocab_to_int.get(word, 0) for word in text.split()[:self.max_length]]\n",
    "    return np.array([0]*(0 if len(text_int) > self.max_length else self.max_length - len(text_int)) + text_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(\" \".join(reviews).split(\" \"))\n",
    "encoder = Encoder(vocab)\n",
    "features = np.array([encoder.encode(review) for review in reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to 1s and 0s for 'positive' and 'negative'\n",
    "labels = np.array([1 if label == 'positive' else 0 for label in labels])\n",
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you build features correctly, it should look like that cell output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 57095, 20557, 56762,\n",
       "        39375, 65062, 30691, 38375,  3424, 66180, 66596, 47425, 17325,\n",
       "         8538, 55536, 21725, 22600, 31630, 11246,  9023, 33005, 51552,\n",
       "        55536, 67162, 38375, 40378,  3950, 55979, 47425, 71680, 40166,\n",
       "        62761, 68329, 19067, 19860, 38151, 57095, 20557,  8878, 33330,\n",
       "        56762, 54647,  8013, 19067, 50188, 73844, 56762, 67162, 38375,\n",
       "        47425, 63398, 19067, 65799, 24117, 47425, 55994, 51593, 62632,\n",
       "        54824, 32680, 68614, 11905, 57127, 61883, 67162, 73007, 47425,\n",
       "        31545],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 49669,  4462, 39375,\n",
       "        25716, 62632, 45788, 13142, 71412, 61208, 39375, 37130, 38375,\n",
       "        36457, 64886, 18917, 39375, 25414, 31542, 38151, 56762, 39375,\n",
       "        69772],\n",
       "       [32663, 20000, 72249, 55536, 20791, 14514, 49334, 45788, 30279,\n",
       "         6103, 56754, 61208,  3950, 49214,  8070, 39375,  5352, 19067,\n",
       "        44795, 52572, 71445, 47425, 37655, 38151, 27757, 60931, 11499,\n",
       "         5813, 62632,  4172, 63629, 69708, 32128, 19067,  9023, 26540,\n",
       "        20000, 16702, 61208, 47425,  2317, 38375, 26680, 10592, 61977,\n",
       "         4462, 47425, 59679, 55536,  3723, 39375, 13915, 13011,  8449,\n",
       "        32292, 11246, 43533, 51552, 55536, 45317, 47425, 28050, 71445,\n",
       "        73657, 37935, 31533, 19067, 36013, 73729, 47425, 18735, 12839,\n",
       "        20000, 32292, 13412, 32793, 41476,   894, 69315, 19067, 31085,\n",
       "        64760, 71445, 47425, 31473, 38375, 17223, 17223, 49214, 60962,\n",
       "        13412,  3045, 27757, 63739, 39375,   294, 19067, 53230, 71445,\n",
       "        47425],\n",
       "       [52765, 36457, 55536, 39375, 24380, 43934,   306, 50418, 56762,\n",
       "        14537, 64760, 18917, 47676, 21770, 51552, 32210, 19067, 11438,\n",
       "        43941,  7503,  1496, 66932, 39034, 62632, 56762,  1000, 68418,\n",
       "        39375, 19608,  4462, 57338,  8878, 19067,  2813, 59638, 55979,\n",
       "        63636,  4462,  3424, 38761, 61695, 19067, 47425, 28998, 55536,\n",
       "        39375, 21081, 22390, 71445, 37837, 56762,  1496, 24013, 70408,\n",
       "        24776, 52066, 11404, 69783, 38375, 47425,   306, 60908, 61090,\n",
       "        58151, 55536, 18363, 49214, 15058, 35428, 47425, 50418, 56762,\n",
       "        32880,  5504, 71729, 47425, 30257,  6451, 30689, 13796, 58075,\n",
       "         2813, 20126, 58102,  8878, 60240, 69223, 26653, 13984, 72295,\n",
       "          832, 62632, 12200, 47425, 43094, 39693, 64886, 18917, 57941,\n",
       "        31546],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,  5986, 23498, 59899, 71729, 26976, 49255,\n",
       "        22288, 38375, 64040, 70508, 51003, 25803, 47613, 46746, 47962,\n",
       "        49256, 43937, 57831, 59808, 55979, 41617,   427, 65570, 24345,\n",
       "        19067, 33358, 38375, 47425, 51436, 71445, 58781, 56762, 39375,\n",
       "        67731, 55536,   186, 55536, 66142, 55979, 70505, 48987, 38375,\n",
       "        47425, 48222, 71445, 33839, 56762, 22390, 49378, 38375, 60979,\n",
       "        38761, 34042,  4462, 38761, 39375, 13636,  3712, 64886,  2813,\n",
       "        65147],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        65458, 25950, 56527, 44372, 47613, 11625, 43547, 13450, 40378,\n",
       "        30123, 71445, 66596, 17293, 32072, 71445, 47425, 10953,  4462,\n",
       "        47425,  1424, 28000, 38375, 65458, 56644, 24055, 19067, 51101,\n",
       "         4462, 57632, 24693, 39642, 71699, 47425, 28901, 18917, 11404,\n",
       "         1424, 25716, 38375, 28200, 47425, 17416, 59808, 15915, 18117,\n",
       "        55536, 38761, 19270, 47425, 36577, 66596, 56411, 38375,  3424,\n",
       "        30131],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 65458, 56762, 13870,\n",
       "        47425, 26680, 68991, 25950, 43741, 47425, 31618, 16420, 38375,\n",
       "        12709, 54470, 10994, 38375,  3424, 67813,  4940,  7180, 39375,\n",
       "        35925, 50802,  4462, 32663, 24441, 60102, 46041, 72584, 60305,\n",
       "        37865],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0, 14175, 37595, 47613, 58817, 65458, 56762, 70203, 19067,\n",
       "          894,  6103, 58328, 25950, 49214, 53605, 32793, 57929, 46746,\n",
       "        22019, 64886, 50110, 66596, 47425, 57195, 20361, 10592, 30131,\n",
       "        25947, 57127, 22706, 64886, 43937,  4940, 65615, 38375,   111,\n",
       "        47425, 31542,  6288, 43937, 18253, 36924, 64532, 50083, 65458,\n",
       "        49669, 56762, 43411, 48395, 19067, 65615, 38375, 47425, 64262,\n",
       "         4462, 39375, 41946, 71546, 64532, 69864, 38375, 47425, 32737,\n",
       "        71879],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0, 65458, 56762,  4940, 47425, 19574, 66689, 31618, 25950,\n",
       "        38375,  3424, 64532, 54647, 41443, 27150, 73844, 26680,  4462,\n",
       "         2813, 60606, 43937, 12168, 24145, 39375, 38704, 38151, 64532,\n",
       "        65678],\n",
       "       [24693, 47613, 64532, 49179, 40378, 56417, 56553, 68329, 24572,\n",
       "        19067, 47425, 53859, 19067, 32680, 17347, 38375,  3424, 64532,\n",
       "         8827,  4462, 47837, 60606, 47613, 17133, 18917, 40378, 56417,\n",
       "        49214, 65458, 64532, 47425, 30340,  8827, 32465, 45658, 64886,\n",
       "         4462, 38375, 58276,  4915, 47613, 24145,  8070, 49256, 17347,\n",
       "         1565,  3723, 23506, 43937, 47613, 30131, 46746, 69948, 64886,\n",
       "        47425, 45464,  4462, 40378, 33005, 65064,  3424, 38375, 60962,\n",
       "        39375, 40630, 67868, 43937, 55839, 27547, 16911,  4462,  8878,\n",
       "        66619, 43937, 15725, 73748, 38375,  2650, 51180, 56762,  8827,\n",
       "         4462, 40378, 60831, 31959, 49214, 17347, 56762, 71729, 68654,\n",
       "        47425, 36824, 16911,  4462, 68258,  4462,  2813,  2666, 38375,\n",
       "        55979]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "> **Exercise:** Create the training, validation, and test sets here. You'll need to create sets for the features and the labels, `X_train` and `y_train` for example. Define a split fraction, `split_frac` as the fraction of data to keep in the training set. Usually this is set to 0.8 or 0.9. The rest of the data will be split in half to create the validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the train+val into training and validation sets (80% train, 20% val)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(X_train.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(X_val.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model using Subclassing\n",
    "\n",
    "Here, we'll build the model. First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `batch_size`: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network itself, we'll be passing in our 200 element long review vectors. Each batch will be `batch_size` vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Create the `inputs_`, `labels_`, and drop out `keep_prob` placeholders using `tf.placeholder`. `labels_` needs to be two-dimensional to work with some functions later.  Since `keep_prob` is a scalar (a 0-dimensional tensor), you shouldn't provide a size to `tf.placeholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1  # Vocabulary size, +1 for padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, seq_len, embedding_dim=128, lstm_units=128, dropout_rate=0.2):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_len)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, dropout=dropout_rate)\n",
    "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # Forward pass\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    multiple                  9481600   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              multiple                  131584    \n",
      "                                                                 \n",
      " dense_13 (Dense)            multiple                  129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,613,313\n",
      "Trainable params: 9,613,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = SentimentAnalysisModel(vocab_size=n_words, seq_len=SEQ_LEN)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, SEQ_LEN))\n",
    "\n",
    "# Print model summary to check the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 15s 22ms/step - loss: 0.5534 - accuracy: 0.7187 - val_loss: 0.5539 - val_accuracy: 0.7144\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.4713 - accuracy: 0.7832 - val_loss: 0.5726 - val_accuracy: 0.7264\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2799 - accuracy: 0.8918 - val_loss: 0.4235 - val_accuracy: 0.8064\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.1483 - accuracy: 0.9475 - val_loss: 0.3922 - val_accuracy: 0.8348\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0814 - accuracy: 0.9736 - val_loss: 0.4933 - val_accuracy: 0.8484\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0458 - accuracy: 0.9861 - val_loss: 0.5770 - val_accuracy: 0.8368\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0287 - accuracy: 0.9912 - val_loss: 0.6722 - val_accuracy: 0.8368\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 0.6874 - val_accuracy: 0.8396\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.7756 - val_accuracy: 0.8328\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.8812 - val_accuracy: 0.8048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy cannot be improved further. Let us save the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_analysis\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_analysis\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('sentiment_analysis', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 13ms/step - loss: 0.8754 - accuracy: 0.8088\n",
      "Test Loss: 0.8754271268844604\n",
      "Test Accuracy: 0.8087999820709229\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try for simple tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.713653]]\n",
      "[[0.1438838]]\n"
     ]
    }
   ],
   "source": [
    "tests = [\"I love this movie!\", \"This was the worst experience ever.\"]\n",
    "\n",
    "single_input = encoder.encode(tests[0]).reshape(1, 200)\n",
    "prediction = model.predict(single_input)\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "single_input = encoder.encode(tests[1]).reshape(1, 200)\n",
    "prediction = model.predict(single_input)\n",
    "print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
