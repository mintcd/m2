\section{Stochastic Differential Equations}

\begin{frame}{Theory Roadmap}
    \begin{figure}
        \centering
        \includegraphics[width =\textwidth]{img/theory-outline.png}
        \label{fig:enter-label}
    \end{figure}
\end{frame}

\subsection{Probability Space}

\begin{frame}{Probability Space}
A probability space is a triple $(\Omega, \mathcal{F}, P)$, where

\begin{enumerate}
    \item $\Omega$ is a set
    \item $\mathcal{F}$ is a $\sigma$-algebra containing $\Omega$
    \item $P:\Omega\to[0,1]$ is a probability measure
\end{enumerate}
\end{frame}

\subsection{Random Variable}

\begin{frame}{Random Variable}
The Borel $\sigma$-algebra on $\mathbb{R}^n$ is the $\sigma$-algebra generated by open subsets of $\mathbb{R}^n$, denoted by $\mathcal{B}$.\\
Given $(\Omega,\mathcal{F},P)$. A function $\mathbf{X}:\Omega\to\mathbb{R}^n$ is called a random variable if
$$\mathbf{X}^{-1}(B)\in\mathcal{F},\,\,\forall B\in\mathcal{B}.$$
$\mathbf{X}$ is also called $\mathcal{F}$-measurable.

\textit{Example.} Toss three coins of values $10p, 20p$ and $50p$. Define
\begin{enumerate}
    \item $\Omega=\{HHH,HHT,HTH,HTT,THH,THT,TTH,TTT\}$.
    \item $\mathcal{F}=2^\Omega$ (the power set of $\Omega$).
    \item $X$ is the total value of head-up coins.
\end{enumerate}
\end{frame}

\subsection{Conditional Expectation}

\begin{frame}{Expectation Conditioned on an Event}
    Given a probability space $(\Omega,\mathcal{F},P)$ and an event $A\in\mathcal{F}$ with $P(A)>0$. The conditional expectation of a random variable $X$ given $A$ is

    $$\mathbb{E}(X|A)=\dfrac{1}{P(A)}\int_A X\,\mathrm{d}P.$$
\end{frame}

\begin{frame}{Expectation Conditioned on an Event}
    Toss three coins of values $10p, 20p$ and $50p$. Let $X$ be the random variable of the total value of head-up coins. What is the expectation of $X$ given that two coins landed heads up.\\

    We have $A=\{HHT, HTH, THH\}$, then elements of $A$ has probability $\dfrac{1}{8}$ and $P(A)=\dfrac{3}{8}$. Therefore,

    $$\mathbb{E}(X|A) = \dfrac{1}{\frac{3}{8}}\left(\dfrac{1}{8}\times 30 + \dfrac{1}{8}\times 60 + \dfrac{1}{8}\times 70\right)=53\dfrac{1}{3}.$$
\end{frame}

\begin{frame}{Expectation Conditioned on a Random Variable}
    Let $X,Y$ be random variables, then the conditional expectation of $X$ given $Y$ is defined to be the random variable $\mathbb{E}(X|Y)$ such that
    \begin{enumerate}
        \item $\mathbb{E}(X|Y)$ is $\sigma(Y)$-measurable;
        \item For any event $A\in\sigma(Y)$,
        \begin{align}
            \int_A \mathbb{E}(X|Y)\,\mathrm{d}P = \int_A X\,\mathrm{d}P.
        \end{align}
    \end{enumerate}
    For discrete $Y$, we can write (1) as

    $$\mathbb{E}(X|Y)(\omega) = \mathbb{E}(X|Y=Y(\omega)),\,\,\forall \omega\in\sigma(Y).$$
\end{frame}

\begin{frame}{Expectation Conditioned on a Random Variable}
 Toss three coins of values $10p, 20p$ and $50p$. Let $X$ be the random variable of the total value of head-up coins. What is the expectation of $X$ given the total amount $Y$ shown by the $10p$ and $20p$ coins only.\\

 $Y$ takes $4$ possible value: $0,10,20$ and $30$. Similarly to expectation conditioned on an event, we can calculate\\
 $$\mathbb{E}(X|Y=0)=25,\,\,\,\,\,\mathbb{E}(X|Y=10)=35,$$
 $$\mathbb{E}(X|Y=20)=45,\,\,\,\,\,\mathbb{E}(X|Y=30)=55.$$

 Therefore, $(\mathbb{E}|Y)(\omega)=\begin{cases}
     25,\,\,\text{ if } Y(\omega)=0\\
     35,\,\,\text{ if } Y(\omega)=10\\
     45,\,\,\text{ if } Y(\omega)=20\\
     55,\,\,\text{ if } Y(\omega)=30.\\
 \end{cases}$
\end{frame}

\begin{frame}{Expectation Conditioned on a $\sigma$-algebra}
    Conditional expectation does not depend on the random variable itself, but the $\sigma$-algebra it generates, as the theorem below.\\
    
    \textbf{Theorem.} If $\sigma(Y)=\sigma(Z)$, then $\mathbb{E}(X|Y)=\mathbb{E}(X|Z), a.s.$\\

    Hence we can define $\mathbb{E}(X|\,\mathcal{U}) = \mathbb{E}(X|Y)$ where $\sigma(Y)=\mathcal{U}$.\\
\end{frame}

\subsection{Stochastic Process}
\begin{frame}{Stochastic Process}
    \begin{enumerate}
    \item A collection of random variables $\{X(t)\,|\,t\ge0\}$ is called a stochastic process.
    \item For each point $\omega\in\Omega$, the mapping $t\mapsto X(t,w)$ is the corresponding sample path.
\end{enumerate}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{img/stochastic.png}
    \caption{Two continuous sample paths of a stochastic process}
\end{figure}
\end{frame}

\begin{frame}{Brownian Motion}
    \begin{definition}
    A real-valued stochastic process $\{W_t\}_{t\ge0}$ is called a Wiener process (or Brownian motion) if
    \begin{enumerate}
        \item $W_0=0$ a.s. 
        \item $W(t)-W(s)$ is $\mathcal{N}(0,t-s)$ for $t\ge s\ge 0.$
        \item For all times $0<t_1<t_2<...<t_n$, the random variables $W(t_1),W(t_2)-W(t_1),...,W(t_n)-W(t_{n-1})$ are independent.
    \end{enumerate}
\end{definition}
\end{frame}

\subsection{Construction of Stochastic Integrals}

\begin{frame}{Construction of Stochastic Integrals}
    The integral $\int\limits_0^T G\,\mathrm{d}W$, where $G$ is a $1D$ stochastic process, $W$ is the $1D$-Brownian motion is defined as a Riemann sum. 
    \begin{enumerate}
        \item In $[0,T]$, let $P=\{t_0,...,t_k\}$ where $0=t_0<t_1<...<t_k=T$ be a partition.
        \item Denote $|P|=\max\{t_{i+1}-t_i\,|\, i=0,...,k-1\}$.
        \item Fix $\lambda\in[0,1]$. Let $\tau_i = \lambda t_{i} + (1-\lambda)t_{i+1},\,\,i=0,...,k-1$
        \item Let $R(\lambda, P)=\sum\limits_{i=0}^{k-1}G(\tau_i)(W(t_{i+1})-W(t_i))$.
    \end{enumerate}
    
    
    It is prove that
    $$\lim_{\substack{k\to\infty\\ |P|\to 0}}R(\lambda, P)$$
    exists by
    \begin{enumerate}
        \item Proving the existence in a simpler class of $G$;
        \item Approximating in general case.
    \end{enumerate}
\end{frame}

\subsection{It么's rules}
\begin{frame}{It么's rules}
Recall the form of a stochastic differential equation 
$$\mathrm{d}X = f\,\mathrm{d}t + G\,\mathrm{d}W.$$
\begin{enumerate}
    \item The chain rule is the calculation of $\mathrm{d}u(X,t)$.
    \item The product rule is the calculation of $\mathrm{d}(X_1X_2)$. 
\end{enumerate}
\end{frame}

\subsection{Markov Properties of SDE solutions and Backward SDEs}
\begin{frame}{Markov Properties of SDE solutions Backward SDEs}

\begin{enumerate}
    \item An SDE solution can be represented as a Markov process.
    \item The backward SDE can be defined.
    \item Kolmogorov's forward and backward equations are inferred from the It么's rules.
    \item The theorem of reverse diffusion (Anderson, 1982) is prove by Kolmogorov's Equations.
\end{enumerate}
\end{frame}

\begin{frame}{Difficulties}
    \begin{enumerate}
    \item Generator of an It么 diffusion.
        \item Kolmogorov's theorems of extension and continuity.
    \end{enumerate}
\end{frame}