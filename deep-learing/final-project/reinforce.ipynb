{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from copy import deepcopy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = pd.read_csv(r\"complexity_by_hour.csv\")\n",
    "tasks['task_complexity'] = tasks['task_complexity'] / 60\n",
    "tasks = tasks.to_dict(orient='records')\n",
    "\n",
    "N_total_nodes = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSchedulingEnv(gym.Env):\n",
    "    def __init__(self, N_total_nodes, total_tasks):\n",
    "        super(TaskSchedulingEnv, self).__init__()\n",
    "\n",
    "        self.waiting_capacity = 100  # Maximum waiting tasks\n",
    "        self.N_total_nodes = N_total_nodes\n",
    "        self.total_tasks = deepcopy(total_tasks)\n",
    "\n",
    "        self.upcoming_tasks = self.total_tasks.copy()\n",
    "        self.waiting_tasks = []\n",
    "        self.executing_tasks = []\n",
    "        self.available_nodes = self.N_total_nodes\n",
    "        self.current_time = self.total_tasks[0]['hourly_time']\n",
    "\n",
    "        # Observation space: Dict with state and action mask\n",
    "        self.observation_space = spaces.Box(\n",
    "                low=0, high=np.inf, shape=(1 + self.waiting_capacity,), dtype=np.float32\n",
    "            )\n",
    "        # Action space: Discrete (number of nodes + 1 for action 0)\n",
    "        self.action_space = spaces.Discrete(self.N_total_nodes + 1)\n",
    "\n",
    "        self.state = np.array([self.available_nodes] + [0] * self.waiting_capacity, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.upcoming_tasks = self.total_tasks.copy()\n",
    "        self.waiting_tasks = []\n",
    "        self.executing_tasks = []\n",
    "        self.available_nodes = self.N_total_nodes\n",
    "        self.current_time = self.total_tasks[0]['hourly_time']\n",
    "\n",
    "        padded_waiting_times = [0] * self.waiting_capacity\n",
    "        state = np.array([self.available_nodes] + padded_waiting_times, dtype=np.float32)\n",
    "\n",
    "        return state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Validate the action\n",
    "        if action < 0 or action > self.available_nodes:\n",
    "            return self.state, -100, False, False, {}\n",
    "\n",
    "        # Add new task to waiting list\n",
    "        if self.upcoming_tasks and self.current_time == self.upcoming_tasks[0]['hourly_time']:\n",
    "            self.upcoming_tasks[0]['waiting_time'] = 0\n",
    "            self.waiting_tasks.append(self.upcoming_tasks.pop(0))\n",
    "\n",
    "        # Update executing tasks\n",
    "        for task in self.executing_tasks:\n",
    "            task['complexity'] -= task['nodes_alloc']\n",
    "            if task['complexity'] <= 0:\n",
    "                self.available_nodes += task['nodes_alloc']\n",
    "\n",
    "        self.executing_tasks = [task for task in self.executing_tasks if task['complexity'] > 0]\n",
    "\n",
    "        # Allocate nodes to the first waiting task\n",
    "        if self.waiting_tasks and action > 0:\n",
    "            self.waiting_tasks[0]['nodes_alloc'] = action\n",
    "            self.executing_tasks.append(self.waiting_tasks.pop(0))\n",
    "            self.available_nodes -= action\n",
    "\n",
    "        # Update waiting times\n",
    "        for task in self.waiting_tasks:\n",
    "            task['waiting_time'] += 1\n",
    "\n",
    "        # Compute next state\n",
    "        waiting_times = [task['waiting_time'] for task in self.waiting_tasks]\n",
    "        padded_waiting_times = waiting_times + [0] * (self.waiting_capacity - len(waiting_times))\n",
    "        state = np.array([self.available_nodes] + padded_waiting_times, dtype=np.float32)\n",
    "\n",
    "        # Compute reward\n",
    "        reward = -np.sum(waiting_times)\n",
    "\n",
    "        # Check termination\n",
    "        done = len(self.upcoming_tasks) == 0 and len(self.waiting_tasks) == 0 and len(self.executing_tasks) == 0\n",
    "\n",
    "        self.current_time += 1\n",
    "        action_mask = self._get_action_mask()\n",
    "        return state, reward, done, False, {\"action_mask\": action_mask}\n",
    "\n",
    "    def _get_action_mask(self):\n",
    "        # Generate a binary mask for valid actions\n",
    "        mask = np.zeros(self.N_total_nodes + 1, dtype=np.int32)\n",
    "        for i in range(self.available_nodes + 1):\n",
    "            mask[i] = 1\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [{'hourly_time': 0, 'complexity': 5}, {'hourly_time': 1, 'complexity': 3}]\n",
    "env = TaskSchedulingEnv(N_total_nodes=5, total_tasks=tasks)\n",
    "\n",
    "# Test reset\n",
    "obs = env.reset()\n",
    "print(\"Initial observation:\", obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.16     |\n",
      "|    ep_rew_mean     | -247     |\n",
      "| time/              |          |\n",
      "|    fps             | 466      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Action: 0, Reward: -1\n",
      "Action: 0, Reward: -3\n",
      "Action: 1, Reward: -2\n",
      "Action: 1, Reward: -0.0\n",
      "Action: 0, Reward: -0.0\n"
     ]
    }
   ],
   "source": [
    "class ActionMaskWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset()\n",
    "\n",
    "# Initialize the environment\n",
    "tasks = tasks[:10]\n",
    "env = TaskSchedulingEnv(N_total_nodes=5, total_tasks=tasks)\n",
    "wrapped_env = ActionMaskWrapper(env)\n",
    "\n",
    "# Train the PPO agent\n",
    "model = PPO(\"MlpPolicy\", wrapped_env, verbose=1)\n",
    "model.learn(total_timesteps=100)\n",
    "\n",
    "# Test the agent\n",
    "obs, _ = wrapped_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = wrapped_env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
